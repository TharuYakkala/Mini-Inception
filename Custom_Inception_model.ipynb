{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Inception Model from scratch\n",
    "\n",
    "Author: Tharindu Yakkala\n",
    "\n",
    "References:\n",
    "Going Deeper with Convolutions\n",
    "https://arxiv.org/abs/1409.4842"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Building Blocks of the Inception Layer\n",
    "\"\"\"\n",
    "\n",
    "### 1x1 Convolution block -> 3x3 Conv\n",
    "class Conv1x1_3x3(nn.Module):\n",
    "    def __init__(self, input_channels:int , mid_channels: int, output_channels:int) -> torch.Tensor:\n",
    "        \"\"\"Instantiate a Convblock that takes image through a 1x1 Conv\n",
    "        and reduced channels to mid_channels, then through a 3x3 Conv2d\n",
    "        block which outputs 'output_channels'\n",
    "\n",
    "        Args:\n",
    "            input_channels (int): Image imput channels.\n",
    "            mid_channels (int): mid channels for the 1x1 conv.\n",
    "            output_channels (int): out channels for the 3x3 conv.\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, output_channels, x, y)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv_block3x = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_channels, out_channels=mid_channels, kernel_size=1),\n",
    "            nn.Conv2d(in_channels=mid_channels, out_channels=output_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv_block3x(x)\n",
    "    \n",
    "\n",
    "### 1x1 Convolution block -> 5x5 Conv\n",
    "class Conv1x1_5x5(nn.Module):\n",
    "    def __init__(self, input_channels: int, mid_channels: int, output_channels: int) -> torch.Tensor:\n",
    "        \"\"\"Instiantiate a convblock, it takes an input image of input_channels\n",
    "        into a 1x1 conv and reduces channels to 'mid_channels' then into a\n",
    "        5x5 conv that outputs 'output_channels'\n",
    "\n",
    "        Args:\n",
    "            input_channels (int): Image input channels.\n",
    "            mid_channels (int): 1x1 conv output channels.\n",
    "            output_channels (int): 5x5 conv output channels.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Conv block output of shape (batch_size, output_channels, x, y).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv_block5x = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_channels, out_channels=mid_channels, kernel_size=1),\n",
    "            nn.Conv2d(in_channels=mid_channels, out_channels=output_channels, kernel_size=5, padding=2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv_block5x(x)\n",
    "\n",
    "class Pool3x3_Conv1x1(nn.Module):\n",
    "    def __init__(self, input_channels: int, output_channels:int) -> torch.Tensor:\n",
    "        \"\"\"Instantiate a convblock that takes takes image through a MaxPool 3x3 and \n",
    "        maintains same image shape, then into a 1x1 conv block that outputs 'output_channels'\n",
    "\n",
    "        Args:\n",
    "            input_channels (int): Image input channels.\n",
    "            output_channels (int): Image output channels.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Tensor of shape (batch_size, output_channels, x, y)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.pool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_channels=input_channels, out_channels=output_channels, kernel_size=1, stride=1, padding=0)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.pool_conv(x)\n",
    "        \n",
    "class Inception_block(nn.Module):\n",
    "    def __init__(self, input_channels: int, x1_out: int, x3_mid: int, x3_out: int, x5_mid: int, x5_out: int, pool_out: int) -> torch.Tensor:\n",
    "        \"\"\"Instantiate an inception block that contains 4 neural networks in one.\n",
    "        Args:\n",
    "            input_channels (int): Input channels of image.\n",
    "            x1_out (int): Output channels of the 1x1 conv (block1).\n",
    "            x3_mid (int): Mid channels of the 1x1conv_3x3conv (block2).\n",
    "            x3_out (int): Output channels of the 1x1conv_3x3conv (block2).\n",
    "            x5_mid (int): Mid channels of the 1x1Conv_5x5Conv (block3).\n",
    "            x5_out (int): Output channels of the 1x1Conv_5x5Conv (block3).\n",
    "            pool_out (int): Output channels of the 3x3Pool_1x1Conv (block4).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Tensor of shape (batch_size, x1_out + x3_out + x5_out + pool_out, x, y)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Conv2d(in_channels=input_channels, out_channels=x1_out, kernel_size=1, stride=1, padding=0)\n",
    "        self.block2 = Conv1x1_3x3(input_channels=input_channels, mid_channels=x3_mid, output_channels=x3_out)\n",
    "        self.block3 = Conv1x1_5x5(input_channels=input_channels, mid_channels=x5_mid, output_channels=x5_out)\n",
    "        self.block4 = Pool3x3_Conv1x1(input_channels=input_channels, output_channels=pool_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        block1_out = self.block1(x)\n",
    "        block2_out = self.block2(x)\n",
    "        block3_out = self.block3(x)\n",
    "        block4_out = self.block4(x)\n",
    "        \n",
    "        # contatination on channel dim, bring the inner neural nets and combine them.\n",
    "        return torch.concat([block1_out, block2_out, block3_out, block4_out], dim=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The mini-inception model\n",
    "\"\"\"\n",
    "\n",
    "class inception_mini(nn.Module):\n",
    "    def __init__(self, image_channels: int) -> torch.Tensor:\n",
    "        \"\"\"Instantiate a custom inception model for binary classification.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Tensor of shape (batch_size, 1), output logits.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # first conv layer\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=image_channels, out_channels=192,  kernel_size=3, stride=1),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "        )\n",
    "        \n",
    "        # first inception layer, consisting of 4 layers in one.\n",
    "        self.inception1 = Inception_block(input_channels=192,\n",
    "                                          x1_out=64,\n",
    "                                          x3_mid=96,\n",
    "                                          x3_out=128,\n",
    "                                          x5_mid=16,\n",
    "                                          x5_out=32,\n",
    "                                          pool_out=32)\n",
    "        \n",
    "        # second inception layer, also 4 layers in one.\n",
    "        self.inception2 = Inception_block(input_channels=256,\n",
    "            x1_out=64,\n",
    "            x3_mid=96,\n",
    "            x3_out=128,\n",
    "            x5_mid=16,\n",
    "            x5_out=32,\n",
    "            pool_out=32\n",
    "        ) \n",
    "        \n",
    "        # adding bottleneck layer to further optimize speed\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=64, kernel_size=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "        )\n",
    "        \n",
    "        self.adaptive = nn.Sequential(\n",
    "              nn.AdaptiveAvgPool2d((1,1)),\n",
    "              nn.Flatten(start_dim=1)\n",
    "        )\n",
    "             \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(in_features=64, out_features=10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        x1 = self.inception1(x1)\n",
    "        x1 = self.inception2(x1)\n",
    "        x1 = self.bottleneck(x1)\n",
    "        x1 = self.adaptive(x1)\n",
    "        return self.out(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = inception_mini(image_channels=1) #channels=1 for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from torchinfo import summary\n",
    "except:\n",
    "    !pip install torchinfo\n",
    "    from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "inception_mini                           [32, 10]                  --\n",
       "├─Sequential: 1-1                        [32, 192, 13, 13]         --\n",
       "│    └─Conv2d: 2-1                       [32, 192, 26, 26]         1,920\n",
       "│    └─BatchNorm2d: 2-2                  [32, 192, 26, 26]         384\n",
       "│    └─ReLU: 2-3                         [32, 192, 26, 26]         --\n",
       "│    └─MaxPool2d: 2-4                    [32, 192, 13, 13]         --\n",
       "├─Inception_block: 1-2                   [32, 256, 13, 13]         --\n",
       "│    └─Conv2d: 2-5                       [32, 64, 13, 13]          12,352\n",
       "│    └─Conv1x1_3x3: 2-6                  [32, 128, 13, 13]         --\n",
       "│    │    └─Sequential: 3-1              [32, 128, 13, 13]         129,248\n",
       "│    └─Conv1x1_5x5: 2-7                  [32, 32, 13, 13]          --\n",
       "│    │    └─Sequential: 3-2              [32, 32, 13, 13]          15,920\n",
       "│    └─Pool3x3_Conv1x1: 2-8              [32, 32, 13, 13]          --\n",
       "│    │    └─Sequential: 3-3              [32, 32, 13, 13]          6,176\n",
       "├─Inception_block: 1-3                   [32, 256, 13, 13]         --\n",
       "│    └─Conv2d: 2-9                       [32, 64, 13, 13]          16,448\n",
       "│    └─Conv1x1_3x3: 2-10                 [32, 128, 13, 13]         --\n",
       "│    │    └─Sequential: 3-4              [32, 128, 13, 13]         135,392\n",
       "│    └─Conv1x1_5x5: 2-11                 [32, 32, 13, 13]          --\n",
       "│    │    └─Sequential: 3-5              [32, 32, 13, 13]          16,944\n",
       "│    └─Pool3x3_Conv1x1: 2-12             [32, 32, 13, 13]          --\n",
       "│    │    └─Sequential: 3-6              [32, 32, 13, 13]          8,224\n",
       "├─Sequential: 1-4                        [32, 64, 6, 6]            --\n",
       "│    └─Conv2d: 2-13                      [32, 64, 13, 13]          16,448\n",
       "│    └─BatchNorm2d: 2-14                 [32, 64, 13, 13]          128\n",
       "│    └─ReLU: 2-15                        [32, 64, 13, 13]          --\n",
       "│    └─MaxPool2d: 2-16                   [32, 64, 6, 6]            --\n",
       "├─Sequential: 1-5                        [32, 64]                  --\n",
       "│    └─AdaptiveAvgPool2d: 2-17           [32, 64, 1, 1]            --\n",
       "│    └─Flatten: 2-18                     [32, 64]                  --\n",
       "├─Sequential: 1-6                        [32, 10]                  --\n",
       "│    └─Linear: 2-19                      [32, 10]                  650\n",
       "==========================================================================================\n",
       "Total params: 360,234\n",
       "Trainable params: 360,234\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 1.97\n",
       "==========================================================================================\n",
       "Input size (MB): 0.10\n",
       "Forward/backward pass size (MB): 103.84\n",
       "Params size (MB): 1.44\n",
       "Estimated Total Size (MB): 105.38\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(32, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train = MNIST(root=\"./data_mnist\", train=True, download=True, transform=transform)\n",
    "test = MNIST(root=\"./data_mnist\", train=False, download=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "# test one batch before any training\n",
    "get_batch = next(iter(train_dataloader))\n",
    "try:\n",
    "    with torch.inference_mode():\n",
    "        pred_logits = model(get_batch[0])\n",
    "        pred_proba = torch.softmax(pred_logits, dim=1)\n",
    "        pred = torch.argmax(pred_proba, dim=1)\n",
    "        print(pred)\n",
    "except Exception as e:\n",
    "    print(e,\"\\nInput batch didn't pass through model correctly\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
